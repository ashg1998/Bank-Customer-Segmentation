# -*- coding: utf-8 -*-
"""Bank_Customers_Segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18OWDOc6jZiEmiqSOYfzYjum5rG23uisK

## TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS **CASE**
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import seaborn as sns
import numpy as np

credit_card_df = pd.read_csv("/content/4.+Marketing_data.csv")

credit_card_df.info()

credit_card_df.describe()

# CUSTID: Identification of Credit Card holder 
# BALANCE: Balance amount left in customer's account to make purchases
# BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)
# PURCHASES: Amount of purchases made from account
# ONEOFFPURCHASES: Maximum purchase amount done in one-go
# INSTALLMENTS_PURCHASES: Amount of purchase done in installment
# CASH_ADVANCE: Cash in advance given by the user
# PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)
# ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)
# PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)
# CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid
# CASH_ADVANCE_TRX: Number of Transactions made with "Cash in Advance"
# PURCHASES_TRX: Number of purchase transactions made
# CREDIT_LIMIT: Limit of Credit Card for user
# PAYMENTS: Amount of Payment done by user
# MINIMUM_PAYMENTS: Minimum amount of payments made by user  
# PRC_FULL_PAYMENT: Percent of full payment paid by user
# TENURE: Tenure of credit card service for user

# Let's see who made one off purchase of $40761!
credit_card_df[credit_card_df['ONEOFF_PURCHASES'] == 40761.25]

credit_card_df['CASH_ADVANCE'].max()

# Let's see who made the maximum cash advance
credit_card_df[credit_card_df['CASH_ADVANCE'] == 47137.211760000006]

"""## Task 3: Visualize the Dataset"""

#checking the missing value
sns.heatmap(credit_card_df.isnull(),yticklabels=False, cbar=False, cmap="Blues")

(credit_card_df.isnull().sum()/len(credit_card_df))*100

credit_card_df['CREDIT_LIMIT'].fillna(credit_card_df['CREDIT_LIMIT'].mean(), inplace = True)

(credit_card_df.isnull().sum()/len(credit_card_df))*100

credit_card_df['MINIMUM_PAYMENTS'].fillna(credit_card_df['MINIMUM_PAYMENTS'].mean(), inplace = True)

# Fill up the missing elements with mean of the 'MINIMUM_PAYMENT' 
#creditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()

#checking for duplicated entries
credit_card_df.duplicated().sum()

credit_card_df.drop('CUST_ID',axis=1, inplace=True)

credit_card_df.head()

n = len(credit_card_df.columns)
n

plt.figure(figsize=(10,50))
for i in range(len(credit_card_df.columns)):
  plt.subplot(17,1,i+1)
  sns.distplot(credit_card_df[credit_card_df.columns[i]],kde_kws={"color":"b","lw":3, "label":"KDE"}, hist_kws={"color":"g"})
  plt.title(credit_card_df.columns[i])

plt.tight_layout()

f, ax = plt.subplots(figsize = (20, 20))
sns.heatmap(credit_card_df.corr(), annot=True)

"""## Applying K-Means Clustering"""

## Finding Optimal K for K-Means Clustering

scaler = StandardScaler()
credit_card_df_scaled = scaler.fit_transform(credit_card_df)

credit_card_df_scaled.shape

credit_card_df_scaled

score_1 = []
range_values = range(1,20)
for i in range_values:
  kmeans = KMeans(n_clusters=i)
  kmeans.fit(credit_card_df_scaled)
  score_1.append(kmeans.inertia_)

plt.plot(score_1, 'bx-')
plt.title("finding the rigth number of clusters")
plt.xlabel("Clusters")
plt.ylabel("Scores WCSS")

"""## Tasks6 : Applying K-Means Method"""

kmeans  = KMeans(8)
kmeans.fit(credit_card_df_scaled)
labels = kmeans.labels_

labels

kmeans.cluster_centers_.shape

#concatenate the clusters labels to our original dataframe
credit_card_df_cluster = pd.concat([credit_card_df,pd.DataFrame({'clusters':labels})],axis = 1)
credit_card_df_cluster.head()

#plot the histogram of Various clusters
for i in credit_card_df.columns:
  plt.figure(figsize=(35,5))
  for j in range(8):
    plt.subplot(1,8,j+1)
    clusters = credit_card_df_cluster[credit_card_df_cluster['clusters'] == j]
    clusters[i].hist(bins = 20)
    plt.title("{} \nCluster {}".format(i,j))
  
  plt.show()



"""## Task 7: Apply PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS"""

pca = PCA(n_components = 2)
principal_comp = pca.fit_transform(credit_card_df_scaled)
principal_comp

#Create a dataframe with the two components
pca_df = pd.DataFrame(data = principal_comp, columns = ['pca1','pca2'])
pca_df.head()

#Concatenate the clusters labels to the dataframe
pca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)
pca_df.head()

plt.figure(figsize=(10,10))
ax = sns.scatterplot(x="pca1",y="pca2", hue = "cluster", data = pca_df, palette=['red','green','yellow','pink','black','gray','purple','blue'])
plt.show()

"""## Task 9: Apply AutoENCODER(PERFORM DIMENSIONALITY REDUCTION USING AUTOENCODERS)"""

from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from keras.optimizers import SGD

encoding_dim = 7

input_df = Input(shape=(17,))
#Glorot normal initializer (Xavier normal initializer) draws samples from a truncated normal disttibution

x = Dense(encoding_dim, activation='relu')(input_df)
x = Dense(500, activation='relu', kernel_initializer='glorot_uniform')(x)
x = Dense(500, activation='relu', kernel_initializer='glorot_uniform')(x)
x = Dense(2000, activation='relu', kernel_initializer='glorot_uniform')(x)

encoded = Dense(10, activation='relu', kernel_initializer='glorot_uniform')(x)
x = Dense(2000, activation='relu', kernel_initializer='glorot_uniform')(encoded)
x = Dense(500, activation='relu', kernel_initializer='glorot_uniform')(x)

decoded = Dense(17, kernel_initializer= 'glorot_uniform')(x)

#autoencoder
autoencoder = Model(input_df,decoded)

#encoder - used for our dimension reduction
encoder = Model(input_df, encoded)

autoencoder.compile(optimizer = 'adam', loss = 'mean_squared_error')

credit_card_df_scaled.shape

autoencoder.fit(credit_card_df_scaled, credit_card_df_scaled, batch_size = 128, epochs = 25, verbose =1)

autoencoder.save_weights('autoencoder.h5')

pred = encoder.predict(credit_card_df_scaled)

pred.shape



score_2 = []
range_values = range(1,20)
for i in range_values:
  kmeans = KMeans(n_clusters=i)
  kmeans.fit(pred)
  score_2.append(kmeans.inertia_)

plt.plot(score_1, 'bx-')
plt.title("finding the rigth number of clusters")
plt.xlabel("Clusters")
plt.ylabel("Scores WCSS")

plt.plot(score_1,'bx-', color= 'r')
plt.plot(score_2,'bx-', color= 'g')

kmeans = KMeans(4)
kmeans.fit(pred)
labels = kmeans.labels_
y_pred = kmeans.fit_predict(credit_card_df_scaled)

df_cluster_dr = pd.concat([credit_card_df,pd.DataFrame({'clusters':labels})], axis = 1)



pca = PCA(n_components=2)
prin_comp = pca.fit_transform(pred)
pca_df = pd.DataFrame(data = prin_comp, columns =['pca1','pca2'])
pca_df.head()

pca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis = 1)
pca_df.head()

plt.figure(figsize=(10,10))
ax = sns.scatterplot(x="pca1", y="pca2", hue = "cluster", data = pca_df, palette =['red','green','blue','yellow'])
plt.show()